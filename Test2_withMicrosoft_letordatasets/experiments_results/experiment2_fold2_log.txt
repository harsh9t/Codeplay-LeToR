
[+] General Parameters:
LETOR 4.0 dataset: No
Training data:	MQ2008/Fold2/train.txt
Test data:	MQ2008/Fold2/test.txt
Validation data:	MQ2008/Fold2/vali.txt
Ranking method:	LambdaMART
Feature description file:	Unspecified. All features will be used.
Train metric:	NDCG@10
Test metric:	ERR@10
Highest relevance label (to compute ERR): 4
Feature normalization: No
Model file: mymodel_fold2.txt

[+] LambdaMART's Parameters:
No. of trees: 1000
No. of leaves: 10
No. of threshold candidates: 256
Learning rate: 0.1
Stop early: 100 rounds without performance gain on validation data

Reading feature file [MQ2008/Fold2/train.txt]: 0... Reading feature file [MQ2008/Fold2/train.txt]... [Done.]            
(471 ranked lists, 9404 entries read)
Reading feature file [MQ2008/Fold2/vali.txt]: 0... Reading feature file [MQ2008/Fold2/vali.txt]... [Done.]            
(156 ranked lists, 2874 entries read)
Reading feature file [MQ2008/Fold2/test.txt]: 0... Reading feature file [MQ2008/Fold2/test.txt]... [Done.]            
(157 ranked lists, 2933 entries read)
Initializing... [Done]
---------------------------------
Training starts...
---------------------------------
#iter   | NDCG@10-T | NDCG@10-V | 
---------------------------------
1       | 0.5272    | 0.4777    | 
2       | 0.5322    | 0.4826    | 
3       | 0.5403    | 0.4891    | 
4       | 0.5384    | 0.4884    | 
5       | 0.5369    | 0.4856    | 
6       | 0.5359    | 0.4825    | 
7       | 0.5362    | 0.4746    | 
8       | 0.5373    | 0.4769    | 
9       | 0.5412    | 0.4763    | 
10      | 0.5406    | 0.4776    | 
11      | 0.5428    | 0.4876    | 
12      | 0.5433    | 0.4913    | 
13      | 0.5419    | 0.4902    | 
14      | 0.5442    | 0.4924    | 
15      | 0.5453    | 0.489     | 
16      | 0.5447    | 0.4898    | 
17      | 0.5452    | 0.4933    | 
18      | 0.548     | 0.4922    | 
19      | 0.5485    | 0.4885    | 
20      | 0.5495    | 0.4884    | 
21      | 0.5511    | 0.4888    | 
22      | 0.5497    | 0.4887    | 
23      | 0.5519    | 0.4917    | 
24      | 0.5505    | 0.4921    | 
25      | 0.5514    | 0.4934    | 
26      | 0.5525    | 0.4929    | 
27      | 0.5541    | 0.4945    | 
28      | 0.5541    | 0.4933    | 
29      | 0.5545    | 0.4937    | 
30      | 0.555     | 0.4946    | 
31      | 0.5559    | 0.4958    | 
32      | 0.5573    | 0.4961    | 
33      | 0.5568    | 0.4939    | 
34      | 0.558     | 0.4961    | 
35      | 0.5586    | 0.4969    | 
36      | 0.5576    | 0.4953    | 
37      | 0.5585    | 0.4953    | 
38      | 0.5593    | 0.4962    | 
39      | 0.562     | 0.4968    | 
40      | 0.5624    | 0.4956    | 
41      | 0.5628    | 0.4956    | 
42      | 0.5627    | 0.4963    | 
43      | 0.5635    | 0.498     | 
44      | 0.5643    | 0.4974    | 
45      | 0.5651    | 0.4982    | 
46      | 0.5656    | 0.4983    | 
47      | 0.5661    | 0.4982    | 
48      | 0.5661    | 0.4972    | 
49      | 0.568     | 0.4981    | 
50      | 0.5682    | 0.4978    | 
51      | 0.5695    | 0.4968    | 
52      | 0.5702    | 0.4971    | 
53      | 0.5701    | 0.4975    | 
54      | 0.5712    | 0.4982    | 
55      | 0.5719    | 0.498     | 
56      | 0.5727    | 0.4945    | 
57      | 0.5739    | 0.4943    | 
58      | 0.575     | 0.4941    | 
59      | 0.5749    | 0.4958    | 
60      | 0.5762    | 0.4956    | 
61      | 0.5767    | 0.4953    | 
62      | 0.58      | 0.4959    | 
63      | 0.5812    | 0.4957    | 
64      | 0.5819    | 0.4949    | 
65      | 0.5832    | 0.4977    | 
66      | 0.5826    | 0.4972    | 
67      | 0.5823    | 0.4955    | 
68      | 0.5825    | 0.4906    | 
69      | 0.5837    | 0.4908    | 
70      | 0.5837    | 0.4912    | 
71      | 0.584     | 0.4906    | 
72      | 0.5849    | 0.4919    | 
73      | 0.5862    | 0.4907    | 
74      | 0.5874    | 0.4918    | 
75      | 0.5903    | 0.4914    | 
76      | 0.59      | 0.4899    | 
77      | 0.5911    | 0.4904    | 
78      | 0.5921    | 0.4901    | 
79      | 0.593     | 0.4882    | 
80      | 0.5919    | 0.4897    | 
81      | 0.593     | 0.4892    | 
82      | 0.5939    | 0.4866    | 
83      | 0.5947    | 0.4887    | 
84      | 0.5946    | 0.4863    | 
85      | 0.5949    | 0.4851    | 
86      | 0.5952    | 0.4852    | 
87      | 0.5968    | 0.4884    | 
88      | 0.5987    | 0.4903    | 
89      | 0.5994    | 0.4912    | 
90      | 0.5995    | 0.4921    | 
91      | 0.5999    | 0.4935    | 
92      | 0.5985    | 0.4908    | 
93      | 0.5984    | 0.4927    | 
94      | 0.6008    | 0.4933    | 
95      | 0.6017    | 0.4945    | 
96      | 0.6017    | 0.493     | 
97      | 0.6044    | 0.4933    | 
98      | 0.6036    | 0.4928    | 
99      | 0.6054    | 0.493     | 
100     | 0.6058    | 0.4939    | 
101     | 0.6058    | 0.494     | 
102     | 0.6051    | 0.4951    | 
103     | 0.6059    | 0.494     | 
104     | 0.6059    | 0.4905    | 
105     | 0.6074    | 0.4898    | 
106     | 0.6077    | 0.4909    | 
107     | 0.6086    | 0.4912    | 
108     | 0.6098    | 0.4911    | 
109     | 0.61      | 0.4918    | 
110     | 0.6105    | 0.4924    | 
111     | 0.6112    | 0.4929    | 
112     | 0.6117    | 0.4927    | 
113     | 0.6123    | 0.4928    | 
114     | 0.6131    | 0.4923    | 
115     | 0.6122    | 0.4926    | 
116     | 0.6129    | 0.4925    | 
117     | 0.6128    | 0.4934    | 
118     | 0.6134    | 0.4938    | 
119     | 0.6149    | 0.4943    | 
120     | 0.6137    | 0.4944    | 
121     | 0.6141    | 0.4949    | 
122     | 0.6147    | 0.4955    | 
123     | 0.6147    | 0.4966    | 
124     | 0.6161    | 0.4965    | 
125     | 0.6161    | 0.493     | 
126     | 0.617     | 0.4933    | 
127     | 0.616     | 0.4938    | 
128     | 0.616     | 0.4939    | 
129     | 0.616     | 0.4963    | 
130     | 0.6159    | 0.4951    | 
131     | 0.6166    | 0.4954    | 
132     | 0.6185    | 0.495     | 
133     | 0.6182    | 0.495     | 
134     | 0.6178    | 0.4959    | 
135     | 0.6192    | 0.4951    | 
136     | 0.6188    | 0.4958    | 
137     | 0.6186    | 0.4957    | 
138     | 0.6186    | 0.4961    | 
139     | 0.62      | 0.496     | 
140     | 0.6198    | 0.4961    | 
141     | 0.6204    | 0.4956    | 
142     | 0.6214    | 0.4946    | 
143     | 0.621     | 0.4947    | 
144     | 0.6215    | 0.4945    | 
145     | 0.6218    | 0.4946    | 
146     | 0.6222    | 0.4983    | 
147     | 0.6226    | 0.4963    | 
---------------------------------
Finished sucessfully.
NDCG@10 on training data: 0.5656
NDCG@10 on validation data: 0.4983
---------------------------------
ERR@10 on test data: 0.0852

Model saved to: mymodel_fold2.txt
