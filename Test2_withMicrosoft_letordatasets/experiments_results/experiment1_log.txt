
[+] General Parameters:
LETOR 4.0 dataset: No
Training data:	MQ2008/Fold1/train.txt
Test data:	MQ2008/Fold1/test.txt
Validation data:	MQ2008/Fold1/vali.txt
Ranking method:	LambdaMART
Feature description file:	Unspecified. All features will be used.
Train metric:	NDCG@10
Test metric:	ERR@10
Highest relevance label (to compute ERR): 4
Feature normalization: No
Model file: mymodel_rp.txt

[+] LambdaMART's Parameters:
No. of trees: 1000
No. of leaves: 10
No. of threshold candidates: 256
Learning rate: 0.1
Stop early: 100 rounds without performance gain on validation data

Reading feature file [MQ2008/Fold1/train.txt]: 0... Reading feature file [MQ2008/Fold1/train.txt]... [Done.]            
(471 ranked lists, 9630 entries read)
Reading feature file [MQ2008/Fold1/vali.txt]: 0... Reading feature file [MQ2008/Fold1/vali.txt]... [Done.]            
(157 ranked lists, 2707 entries read)
Reading feature file [MQ2008/Fold1/test.txt]: 0... Reading feature file [MQ2008/Fold1/test.txt]... [Done.]            
(156 ranked lists, 2874 entries read)
Initializing... [Done]
---------------------------------
Training starts...
---------------------------------
#iter   | NDCG@10-T | NDCG@10-V | 
---------------------------------
1       | 0.4908    | 0.5351    | 
2       | 0.4949    | 0.5431    | 
3       | 0.4923    | 0.5476    | 
4       | 0.4927    | 0.5462    | 
5       | 0.4948    | 0.5455    | 
6       | 0.4927    | 0.5435    | 
7       | 0.4929    | 0.5403    | 
8       | 0.4943    | 0.5435    | 
9       | 0.494     | 0.5432    | 
10      | 0.4943    | 0.5449    | 
11      | 0.4981    | 0.5424    | 
12      | 0.5013    | 0.5449    | 
13      | 0.5042    | 0.5459    | 
14      | 0.5046    | 0.5421    | 
15      | 0.5063    | 0.5416    | 
16      | 0.5104    | 0.5413    | 
17      | 0.5098    | 0.5451    | 
18      | 0.5135    | 0.5443    | 
19      | 0.5127    | 0.5468    | 
20      | 0.5151    | 0.5449    | 
21      | 0.5169    | 0.5453    | 
22      | 0.5184    | 0.5433    | 
23      | 0.5183    | 0.545     | 
24      | 0.5203    | 0.5418    | 
25      | 0.5212    | 0.5418    | 
26      | 0.5211    | 0.5442    | 
27      | 0.5222    | 0.5417    | 
28      | 0.5236    | 0.5429    | 
29      | 0.5251    | 0.5464    | 
30      | 0.5259    | 0.542     | 
31      | 0.5269    | 0.5427    | 
32      | 0.5279    | 0.5432    | 
33      | 0.5303    | 0.5472    | 
34      | 0.5322    | 0.5477    | 
35      | 0.5328    | 0.5467    | 
36      | 0.5309    | 0.5459    | 
37      | 0.5327    | 0.5456    | 
38      | 0.5348    | 0.5465    | 
39      | 0.5353    | 0.5465    | 
40      | 0.5348    | 0.5441    | 
41      | 0.537     | 0.5443    | 
42      | 0.5362    | 0.5414    | 
43      | 0.538     | 0.5423    | 
44      | 0.5399    | 0.5401    | 
45      | 0.5405    | 0.5388    | 
46      | 0.5412    | 0.5396    | 
47      | 0.5434    | 0.5399    | 
48      | 0.5442    | 0.5424    | 
49      | 0.5452    | 0.5433    | 
50      | 0.5459    | 0.5424    | 
51      | 0.5456    | 0.5413    | 
52      | 0.5459    | 0.5408    | 
53      | 0.5471    | 0.5414    | 
54      | 0.5482    | 0.5429    | 
55      | 0.5479    | 0.5399    | 
56      | 0.5496    | 0.5397    | 
57      | 0.5514    | 0.5397    | 
58      | 0.5522    | 0.5402    | 
59      | 0.5526    | 0.538     | 
60      | 0.5537    | 0.5383    | 
61      | 0.5549    | 0.5382    | 
62      | 0.5552    | 0.5391    | 
63      | 0.5563    | 0.5371    | 
64      | 0.5573    | 0.5374    | 
65      | 0.5593    | 0.5379    | 
66      | 0.5591    | 0.5376    | 
67      | 0.5594    | 0.5368    | 
68      | 0.5614    | 0.5372    | 
69      | 0.5623    | 0.5404    | 
70      | 0.5634    | 0.5416    | 
71      | 0.5643    | 0.5404    | 
72      | 0.5637    | 0.5404    | 
73      | 0.5646    | 0.5387    | 
74      | 0.5655    | 0.5402    | 
75      | 0.5659    | 0.5394    | 
76      | 0.5673    | 0.5398    | 
77      | 0.567     | 0.5392    | 
78      | 0.5676    | 0.5387    | 
79      | 0.5694    | 0.5391    | 
80      | 0.5697    | 0.5384    | 
81      | 0.5707    | 0.5392    | 
82      | 0.5731    | 0.538     | 
83      | 0.5733    | 0.5376    | 
84      | 0.5741    | 0.536     | 
85      | 0.5743    | 0.5353    | 
86      | 0.5738    | 0.535     | 
87      | 0.573     | 0.5374    | 
88      | 0.5751    | 0.5369    | 
89      | 0.5763    | 0.539     | 
90      | 0.5761    | 0.5376    | 
91      | 0.5769    | 0.5374    | 
92      | 0.5795    | 0.539     | 
93      | 0.5798    | 0.5397    | 
94      | 0.5781    | 0.5403    | 
95      | 0.5783    | 0.5397    | 
96      | 0.5811    | 0.54      | 
97      | 0.5811    | 0.54      | 
98      | 0.5817    | 0.5391    | 
99      | 0.5812    | 0.5392    | 
100     | 0.5817    | 0.5379    | 
101     | 0.5834    | 0.5389    | 
102     | 0.5837    | 0.5392    | 
103     | 0.5857    | 0.5401    | 
104     | 0.5865    | 0.5384    | 
105     | 0.5875    | 0.5384    | 
106     | 0.5896    | 0.537     | 
107     | 0.59      | 0.5368    | 
108     | 0.5911    | 0.5397    | 
109     | 0.5912    | 0.5419    | 
110     | 0.5913    | 0.5417    | 
111     | 0.5924    | 0.541     | 
112     | 0.593     | 0.5426    | 
113     | 0.5929    | 0.5432    | 
114     | 0.5942    | 0.5402    | 
115     | 0.5947    | 0.5402    | 
116     | 0.5948    | 0.5391    | 
117     | 0.5949    | 0.5389    | 
118     | 0.5966    | 0.5391    | 
119     | 0.5964    | 0.5406    | 
120     | 0.5988    | 0.5404    | 
121     | 0.5988    | 0.5401    | 
122     | 0.5987    | 0.5423    | 
123     | 0.5994    | 0.5406    | 
124     | 0.5999    | 0.5404    | 
125     | 0.5996    | 0.54      | 
126     | 0.6       | 0.5401    | 
127     | 0.5999    | 0.5403    | 
128     | 0.6002    | 0.5407    | 
129     | 0.6006    | 0.54      | 
130     | 0.6003    | 0.5416    | 
131     | 0.6006    | 0.5412    | 
132     | 0.6002    | 0.5396    | 
133     | 0.6003    | 0.5436    | 
134     | 0.6005    | 0.5443    | 
135     | 0.6008    | 0.5435    | 
---------------------------------
Finished sucessfully.
NDCG@10 on training data: 0.5322
NDCG@10 on validation data: 0.5477
---------------------------------
ERR@10 on test data: 0.0983

Model saved to: mymodel_rp.txt
